{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Nevton de Castro\n",
    "\n",
    "Nome: Eduardo Ghellere "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('auth.pass') as fp:\n",
    "    data = json.load(fp)\n",
    "    \n",
    "    #autentification\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'heineken'\n",
    "\n",
    "#Quantidade de mensagens capturadas:\n",
    "n = 500\n",
    "\n",
    "#Quantidade de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "\n",
    "#Filtro de língua\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():    \n",
    "    if msg.full_text.lower()[0] != 'r' and msg.full_text.lower()[1] != 't' and msg.full_text.lower()[2] != ' ':\n",
    "        msgs.append(msg.full_text.lower())\n",
    "        i += 1                                            #remove todos os retweets, codigo do filtro foi feito em 2018.\n",
    "        if i > n:\n",
    "                break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "## Criando um arquivo excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "                                                           #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "                                                            #fecha o arquivo\n",
    "    writer.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Com o arquivo excel dos tweets, deve-se avaliar um por um manualmente sé caso é relevante ou irrelevante, criando uma coluna denominada \"valor\" e atribuindo em cada linha um valor de 1 para relevantes e 0 para irrelevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "## Separando treinamento e teste na base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numero total de tweets relevantes na base de treinamento: 90\n",
      "\n",
      "numero total de tweets irrelevantes na base de treinamento: 210\n",
      "\n",
      "numero total de tweets relevantes na base de teste: 25\n",
      "\n",
      "numero total de tweets irrelevantes na base de teste: 175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nevton\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\nevton\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\nevton\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\nevton\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dados = pd.read_excel('heineken.xlsx')\n",
    "\n",
    "dados_treinamento = dados.head(300)\n",
    "\n",
    "dados_teste = dados.tail(200)\n",
    "\n",
    "dados_teste.reindex()\n",
    "\n",
    "\n",
    "irrelevantes_trei = dados_treinamento[dados.valor==0] #tweets irrelevantes de treinamento\n",
    "relevantes_trei = dados_treinamento[dados.valor==1] #tweets relevantes de treinamento\n",
    "\n",
    "irrelevantes_teste = dados_teste[dados.valor==0] #tweets irrelevantes de teste\n",
    "relevantes_teste = dados_teste[dados.valor==1]  #tweets relevantes de teste\n",
    "\n",
    "rel_trei = relevantes_trei['Treinamento'].tolist()\n",
    "irr_trei = irrelevantes_trei['Treinamento'].tolist()\n",
    "\n",
    "rel_teste = relevantes_teste['Treinamento'].tolist()\n",
    "irr_teste = irrelevantes_teste['Treinamento'].tolist()\n",
    "\n",
    "print()\n",
    "print('numero total de tweets relevantes na base de treinamento: {0}'.format(len(relevantes_trei)))\n",
    "print()\n",
    "print('numero total de tweets irrelevantes na base de treinamento: {0}'.format(len(irrelevantes_trei)))\n",
    "print()\n",
    "print('numero total de tweets relevantes na base de teste: {0}'.format(len(relevantes_teste)))\n",
    "print()\n",
    "print('numero total de tweets irrelevantes na base de teste: {0}'.format(len(irrelevantes_teste)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "## Limpando os tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fim de melhorar o classificador algumas palavras e caracteres podem ser removidos dos tweets, e outros devem ser mantidos, como por exemplo o \"`#`\", já que o simbálo tem um função nos tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpa(tw):\n",
    "    p = []\n",
    "    for i in tw:\n",
    "        punctuation = '[!\\-.:?;,_+]'       # mantem o `#`\n",
    "        pattern = re.compile(punctuation)\n",
    "        text_subbed = re.sub(pattern, ' ', i.lower())\n",
    "        p.append(text_subbed)\n",
    "\n",
    "    tweets_limpo = [] \n",
    "\n",
    "    for i in p:\n",
    "        product = 'heineken'                   # remove o nome do produto \n",
    "        text_subbed = re.sub(product, ' ', i)\n",
    "        tweets_limpo.append(text_subbed)\n",
    "\n",
    "    rfinal = [] \n",
    "    for i in tweets_limpo:         \n",
    "        e = pd.Series(i.split())    #separa as palavras dos tweets\n",
    "        for i in e:                 \n",
    "            rfinal.append(i) \n",
    "            \n",
    "    df = pd.DataFrame(rfinal)      #cria um dataframe para analisar a frequencia de cada palavra\n",
    "    rdata = df[0].value_counts(True)\n",
    "    \n",
    "    return [tweets_limpo, df, rdata]\n",
    "\n",
    "\n",
    "df_rel =    limpa(rel_trei)[1]\n",
    "df_irr =    limpa(irr_trei)[1]\n",
    "\n",
    "value_rel = limpa(rel_trei)[2]\n",
    "value_irr = limpa(irr_trei)[2]\n",
    "\n",
    "reli_limpos =    limpa(rel_trei)[0]\n",
    "irri_limpos =    limpa(irr_trei)[0]\n",
    "\n",
    "rels_limpos =    limpa(rel_teste)[0]\n",
    "irrs_limpos =    limpa(irr_teste)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classificador_Naive_Bayes(tweets):      \n",
    "    lista =[]\n",
    "    \n",
    "    d=-1\n",
    "    for i in tweets: \n",
    "        t=1\n",
    "        e = pd.Series(i.split())\n",
    "            \n",
    "        for w in e:\n",
    "            \n",
    "            if w in df_rel[0].tolist():\n",
    "                r = value_rel[w]  \n",
    "            \n",
    "            else:\n",
    "                r = 0\n",
    "                \n",
    "            t = r*t\n",
    "        t = t*(115/500)       \n",
    "        o=[]\n",
    "        o.append(t)         \n",
    "        lista.append(o)        \n",
    "        \n",
    "    d = -1\n",
    "    for i in tweets:\n",
    "        t =1\n",
    "        d =d+1\n",
    "        e = pd.Series(i.split())\n",
    "        for w in e: \n",
    "            \n",
    "            \n",
    "            if w in df_irr[0].tolist():\n",
    "                r = value_irr[w]\n",
    "            else:\n",
    "                r = 0\n",
    "                \n",
    "                \n",
    "            t = r*t\n",
    "        t = t*(385/500)\n",
    "        lista[d].append(t)\n",
    "    \n",
    "    qual=[]\n",
    "    m=-1\n",
    "    for i in lista:\n",
    "        m=m+1\n",
    "        if (lista[m][0] > lista[m][1]):\n",
    "            qual.append('relevante')\n",
    "        if (lista[m][1] > lista[m][0]):\n",
    "            qual.append('irrelevante')\n",
    "        if (lista[m][1] == lista[m][0]):\n",
    "            qual.append('indefinido')\n",
    "    return qual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Desempenho do Classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "Como o Classificador permite multiplicar por zero em casos de ausência de palavra, muitos dos tweets recebem zero por cento (0%) de chance de serem relevantes ou irrelevantes, por isso foi criado uma terceira classe de indefinidos.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erros e indefinidos nos tweets relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero total de erros nos relevantes: 1\n",
      "porcentagem de erros: 4%\n",
      "\n",
      "numero total de indefinidos: 23\n",
      "porcentagem de indefinidos: 92%\n",
      "\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "erros_rel = 0\n",
    "ind = 0\n",
    "\n",
    "x = rels_limpos\n",
    "c = Classificador_Naive_Bayes(x)\n",
    "\n",
    "for i in c:\n",
    "    if i == 'irrelevante':\n",
    "        erros_rel  += 1\n",
    "    if i == 'indefinido':\n",
    "        ind += 1\n",
    "\n",
    "print('numero total de erros nos relevantes: {0}'.format(erros_rel))\n",
    "r = int(100*erros_rel/(len(x)))\n",
    "print('porcentagem de erros: {0}%'.format(r))\n",
    "print()\n",
    "print('numero total de indefinidos: {0}'.format(ind))\n",
    "b = int(100*ind/(len(x)))\n",
    "print('porcentagem de indefinidos: {0}%'.format(b))\n",
    "\n",
    "print()\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erros e indefinidos nos tweets irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero total de erros nos irrelevantes: 0\n",
      "porcentagem de erros: 0%\n",
      "\n",
      "numero total de indefinidos: 171\n",
      "porcentagem de indefinidos: 97%\n",
      "\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "erros_rel = 0\n",
    "ind = 0\n",
    "x = irrs_limpos\n",
    "c = Classificador_Naive_Bayes(x)\n",
    "for i in c:\n",
    "    if i == 'relevante':\n",
    "        erros_rel  += 1\n",
    "    if i == 'indefinido':\n",
    "        ind += 1\n",
    "        \n",
    "print('numero total de erros nos irrelevantes: {0}'.format(erros_rel))\n",
    "r = int(100*erros_rel/(len(x)))\n",
    "print('porcentagem de erros: {0}%'.format(r))\n",
    "print()\n",
    "print('numero total de indefinidos: {0}'.format(ind))\n",
    "b = int(100*ind/(len(x)))\n",
    "print('porcentagem de indefinidos: {0}%'.format(b))\n",
    "\n",
    "print()\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# Resolvendo o problema dos indefinidos\n",
    "____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para suavizar o classificador de Naive Bayes deve-se utilizar o `Laplace smoothing` que permite que palavras novas não automaticamente desclassifiquem um tweet como relevante ou irrelevante.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classificador_Naive_Bayes_laplace(tweets):\n",
    "    \n",
    "    lista =[]                  #classifica Naive-bayes com Laplace smooting\n",
    "\n",
    "    d=-1\n",
    "    for i in tweets:\n",
    "        t=1\n",
    "\n",
    "        e = pd.Series(i.split())\n",
    "        \n",
    "        for w in e:\n",
    "            if w in df_rel[0].tolist():\n",
    "                r = (df_rel[0].value_counts()[w]+1)/(len(df_rel)*2 + len(df_irr))\n",
    "            else:\n",
    "                r = 1/(len(df_rel)*2 + len(df_irr))\n",
    "            t = r*t\n",
    "        t=t*(115/500)\n",
    "        o=[]\n",
    "        o.append(t)         \n",
    "        lista.append(o)\n",
    "\n",
    "\n",
    "    d = -1\n",
    "    for i in tweets:\n",
    "        t =1\n",
    "        d =d+1\n",
    "        e = pd.Series(i.split())\n",
    "        for w in e:\n",
    "            if w in df_irr[0].tolist():\n",
    "                r = (df_irr[0].value_counts()[w]+1)/(len(df_irr)*2 + len(df_irr))\n",
    "            else:\n",
    "                r= 1/(len(df_irr)*2 + len(df_rel))\n",
    "            t = r*t\n",
    "        t=t*(385/500)\n",
    "        lista[d].append(t)\n",
    "    classificados=[]\n",
    "    m=-1\n",
    "    for i in lista:\n",
    "        m=m+1\n",
    "        if lista[m][0] > lista[m][1]:\n",
    "            classificados.append('relevante')\n",
    "        if lista[m][0] < lista[m][1]:\n",
    "            classificados.append('irrelevante')\n",
    "            \n",
    "\n",
    "    return classificados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Desempenho do novo Classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "Com o problema dos indefinidos resolvido so resta verificar o desempenho real do classificador final\n",
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero total de erros nos relevantes: 17\n",
      "\n",
      "porcentagem de erros: 68%\n",
      "\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "erros_rel = 0\n",
    "x = rels_limpos\n",
    "c = Classificador_Naive_Bayes_laplace(x)\n",
    "for i in c:\n",
    "    if i != 'relevante':\n",
    "        erros_rel  += 1       #erros na classificao de relevantes\n",
    "\n",
    "print('numero total de erros nos relevantes: {0}'.format(erros_rel))\n",
    "r = int(100*erros_rel/(len(x)))\n",
    "print()\n",
    "print('porcentagem de erros: {0}%'.format(r))\n",
    "print()\n",
    "print(len(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero total de erros nos irrelevantes: 26\n",
      "\n",
      "porcentagem de erros: 14%\n",
      "\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "erros_irr = 0\n",
    "x = irrs_limpos\n",
    "c = Classificador_Naive_Bayes_laplace(x)\n",
    "for i in c:\n",
    "    if i != 'irrelevante':\n",
    "        erros_irr  += 1       #erros na classificao de relevantes\n",
    "\n",
    "print('numero total de erros nos irrelevantes: {0}'.format(erros_irr))\n",
    "r = int(100*erros_irr/(len(x)))\n",
    "print()\n",
    "print('porcentagem de erros: {0}%'.format(r))\n",
    "print()\n",
    "print(len(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "porcentagem de erros: 14%\n",
      "\n",
      "porcentagem de acertos: 86%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "erros_tot = erros_irr + erros_rel\n",
    "total = int(100*erros_tot/300)\n",
    "acertos = 100-total\n",
    "\n",
    "print()\n",
    "print('porcentagem de erros: {0}%'.format(total))\n",
    "print()\n",
    "print('porcentagem de acertos: {0}%'.format(acertos))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Concluindo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "O classificador de Naive Bayes com o Laplace smooting teve uma porcentagem de acertos de 86%. Em comparação com o classificador sem o Laplace smoothing que teve 97% de indefinidos é possível afirmar que a técnica de suavizar o classificador permitindo que palavras \"novas\" não automaticamente desclassifiquem um tweet como relevante ou irrelevante.\n",
    "\n",
    "Assim, é possível dizer que para uma empresa que quer acompanhar os tweets que mencionam suas marcas, o uso desse classificador pode otimizar esse trabalho filtrando tweets irrelevantes, como spams e tweets que apenas mencionam a marca sem oferecer criticas ou elogios. \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
